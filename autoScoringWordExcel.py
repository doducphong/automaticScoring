# import pytesseract
# import torch
# from PIL import Image
# from docx import Document
# from docx.shared import RGBColor
# from transformers import LayoutLMTokenizer, LayoutLMForTokenClassification, ViTFeatureExtractor, ViTForImageClassification

# # Load m√¥ h√¨nh LayoutLM ƒë·ªÉ ki·ªÉm tra b·ªë c·ª•c
# tokenizer = LayoutLMTokenizer.from_pretrained("microsoft/layoutlm-base-uncased")
# layout_model = LayoutLMForTokenClassification.from_pretrained("microsoft/layoutlm-base-uncased")

# # Load m√¥ h√¨nh ViT ƒë·ªÉ ki·ªÉm tra l·ªói h√¨nh ·∫£nh
# feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
# vit_model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224")


# def extract_text_from_docx(doc_path):
#     """ Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file .docx """
#     doc = Document(doc_path)
#     text_data = []
#     for para in doc.paragraphs:
#         text_data.append(para.text)
#     return text_data


# def detect_text_errors(text_data):
#     """ Machine Learning ki·ªÉm tra l·ªói font ch·ªØ, cƒÉn l·ªÅ, ti√™u ƒë·ªÅ """
#     errors = []
#     for i, text in enumerate(text_data):
#         if len(text) > 0 and text.isupper():  # V√≠ d·ª•: Ki·ªÉm tra n·∫øu text vi·∫øt hoa to√†n b·ªô
#             errors.append((i, "L·ªói ti√™u ƒë·ªÅ: Vi·∫øt hoa to√†n b·ªô"))
#         if "  " in text:  # Ki·ªÉm tra kho·∫£ng tr·∫Øng d∆∞ th·ª´a
#             errors.append((i, "L·ªói cƒÉn l·ªÅ: C√≥ kho·∫£ng tr·∫Øng d∆∞"))
#     return errors


# def detect_layout_errors(text_data):
#     """ Deep Learning v·ªõi LayoutLM ƒë·ªÉ ki·ªÉm tra l·ªói b·ªë c·ª•c """
#     inputs = tokenizer(text_data, return_tensors="pt", padding=True, truncation=True)
#     outputs = layout_model(**inputs)
#     logits = outputs.logits
#     predictions = torch.argmax(logits, dim=-1)

#     errors = []
#     for i, pred in enumerate(predictions[0].tolist()):
#         if pred != 0:  # N·∫øu model d·ª± ƒëo√°n c√≥ l·ªói
#             errors.append((i, "L·ªói b·ªë c·ª•c: Heading kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng"))
#     return errors


# def extract_images_from_docx(doc_path):
#     """ Tr√≠ch xu·∫•t h√¨nh ·∫£nh t·ª´ file .docx """
#     doc = Document(doc_path)
#     image_paths = []
#     for i, rel in enumerate(doc.part.rels):
#         if "image" in doc.part.rels[rel].target_ref:
#             image_path = f"image_{i}.png"
#             with open(image_path, "wb") as f:
#                 f.write(doc.part.rels[rel].target_part.blob)
#             image_paths.append(image_path)
#     return image_paths


# def detect_image_errors(image_paths):
#     """ Deep Learning v·ªõi ViT ƒë·ªÉ ki·ªÉm tra l·ªói h√¨nh ·∫£nh """
#     errors = []
#     for image_path in image_paths:
#         image = Image.open(image_path)
#         inputs = feature_extractor(images=image, return_tensors="pt")
#         outputs = vit_model(**inputs)
#         preds = torch.argmax(outputs.logits, dim=-1)
#         if preds != 0:  # N·∫øu model ph√°t hi·ªán l·ªói
#             errors.append((image_path, "L·ªói h√¨nh ·∫£nh: Sai k√≠ch th∆∞·ªõc ho·∫∑c b·ªë c·ª•c"))
#     return errors


# def highlight_errors(doc_path, errors):
#     """ T√¥ m√†u ƒë·ªè l·ªói trong file .docx """
#     doc = Document(doc_path)
#     for index, error_msg in errors:
#         if isinstance(index, int):  # L·ªói vƒÉn b·∫£n
#             para = doc.paragraphs[index]
#             run = para.add_run(f"  (‚ö† {error_msg})")
#             run.font.color.rgb = RGBColor(255, 0, 0)  # ƒê√°nh d·∫•u l·ªói m√†u ƒë·ªè
#         else:  # L·ªói h√¨nh ·∫£nh
#             doc.add_paragraph(f"‚ö† {error_msg} t·∫°i {index}").bold = True

#     output_path = "output_with_errors.docx"
#     doc.save(output_path)
#     print(f"‚úÖ B√°o c√°o l·ªói ƒë√£ l∆∞u t·∫°i: {output_path}")


# def main(doc_path):
#     print("üìÇ ƒêang x·ª≠ l√Ω file:", doc_path)

#     # B∆∞·ªõc 1: Tr√≠ch xu·∫•t vƒÉn b·∫£n
#     text_data = extract_text_from_docx(doc_path)
    
#     # B∆∞·ªõc 2: Ki·ªÉm tra l·ªói vƒÉn b·∫£n (ML + DL)
#     text_errors = detect_text_errors(text_data)
#     layout_errors = detect_layout_errors(text_data)

#     # B∆∞·ªõc 3: Ki·ªÉm tra l·ªói h√¨nh ·∫£nh
#     image_paths = extract_images_from_docx(doc_path)
#     image_errors = detect_image_errors(image_paths)

#     # B∆∞·ªõc 4: Ghi nh·∫≠n l·ªói
#     all_errors = text_errors + layout_errors + image_errors
#     if all_errors:
#         print("‚ùå Ph√°t hi·ªán l·ªói trong t√†i li·ªáu!")
#         for error in all_errors:
#             print(f"- {error[1]} (D√≤ng {error[0]})")
#         highlight_errors(doc_path, all_errors)
#     else:
#         print("‚úÖ T√†i li·ªáu kh√¥ng c√≥ l·ªói!")

    
# # Ch·∫°y ki·ªÉm tra tr√™n file .docx
# file_path = "D:\doAnTuChamDiemFileWordExcel\test.docx"  # ƒê·ªïi th√†nh file c·ªßa b·∫°n
# main(file_path)



# from docx import Document

# doc = Document(r"D:\doAnTuChamDiemFileWordExcel\test.docx")
# for para in doc.paragraphs:
#     para_style = para.style
#     for run in para.runs:
#         font_name = run.font.name if run.font.name else para_style.font.name
#         font_size = run.font.size if run.font.size else para_style.font.size
#         print(f"Font: {font_name}, Size: {font_size.pt}, style: {para.style.name}")

# sections = doc.sections
# for section in sections:
#         print(f"Top: {section.top_margin.cm}, Bottom: {section.bottom_margin.cm}")
#         print(f"Left: {section.left_margin.cm}, Right: {section.right_margin.cm}")

# from transformers import LayoutLMForSequenceClassification, LayoutLMTokenizer

# tokenizer = LayoutLMTokenizer.from_pretrained("microsoft/layoutlm-base-uncased")
# model = LayoutLMForSequenceClassification.from_pretrained("microsoft/layoutlm-base-uncased")

# inputs = tokenizer("Ti√™u ƒë·ªÅ: B√°o c√°o t√†i ch√≠nh", return_tensors="pt")
# outputs = model(**inputs)
# print(outputs.logits)

import zipfile
import xml.etree.ElementTree as ET
from docx import Document
from docx.oxml.ns import qn
import sys
sys.stdout.reconfigure(encoding='utf-8')
import base64
import io
import re

# ƒê·ªãnh nghƒ©a namespace c·∫ßn d√πng
namespaces = {
    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
}


def has_section_break(paragraph):
    """
    Ki·ªÉm tra xem ƒëo·∫°n paragraph c√≥ ch·ª©a section break hay kh√¥ng th√¥ng qua tag <w:sectPr> trong pPr.
    """
    # D√πng xpath ƒë·ªÉ ki·ªÉm tra n·∫øu t·ªìn t·∫°i ph·∫ßn t·ª≠ sectPr trong paragraph's properties
    sectPrs = paragraph._p.xpath('./w:pPr/w:sectPr')
    return len(sectPrs) > 0

def extract_sections(docx_path):
    doc = Document(docx_path)
    
    sections_content = []  # Danh s√°ch ch·ª©a n·ªôi dung t·ª´ng section (danh s√°ch c√°c ƒëo·∫°n vƒÉn)
    current_section = []   # Danh s√°ch t·∫°m ch·ª©a text c·ªßa section hi·ªán t·∫°i

    # Duy·ªát qua t·ª´ng paragraph trong t√†i li·ªáu
    for para in doc.paragraphs:
        # Th√™m n·ªôi dung paragraph v√†o section hi·ªán t·∫°i
        current_section.append(para.text)
        
        # N·∫øu ph√°t hi·ªán section break (c√≥ tag <w:sectPr> trong pPr c·ªßa paragraph)
        if has_section_break(para):
            # L∆∞u l·∫°i section hi·ªán t·∫°i
            sections_content.append(current_section)
            # T·∫°o section m·ªõi cho ph·∫ßn sau
            current_section = []
    
    # Sau khi duy·ªát h·∫øt, n·∫øu c√≤n n·ªôi dung ch∆∞a l∆∞u (ph·∫ßn cu·ªëi kh√¥ng c√≥ section break explicit)
    if current_section:
        sections_content.append(current_section)
    
    return sections_content

def remove_namespace(tag):
    return tag.split('}')[-1] if '}' in tag else tag

def find_wordart_texts(element):
    tag = remove_namespace(element.tag)

    if tag == "shape":
        print(f"[DEBUG] Found <shape> tag with attributes: {element.attrib}")
        for attr, val in element.attrib.items():
            if "gfxdata" in attr.lower():
                print("[DEBUG] Found gfxdata attribute, trying to decode...")
                try:
                    decoded = base64.b64decode(val)
                    with zipfile.ZipFile(io.BytesIO(decoded)) as inner_zip:
                        print(f"[DEBUG] Inner zip file contents: {inner_zip.namelist()}")
                        for name in inner_zip.namelist():
                            if name.endswith(".xml"):
                                inner_xml = inner_zip.read(name).decode("utf-8", errors="ignore")
                                matches = re.findall(r'string="(.*?)"', inner_xml)
                                for text in matches:
                                    print(f"[FOUND WordArt] VƒÉn b·∫£n: {text} (trong {name})")
                except zipfile.BadZipFile:
                    print("[ERROR] gfxdata is not a valid zip file")
    # Duy·ªát ti·∫øp c√°c node con
    for child in element:
        find_wordart_texts(child)

def check_two_columns(docx_path):
    # M·ªü file DOCX d∆∞·ªõi d·∫°ng file ZIP
    with zipfile.ZipFile(docx_path) as docx_zip:
        # ƒê·ªçc n·ªôi dung XML c·ªßa file document.xml
        xml_content = docx_zip.read("word/document.xml")
    
    # Ph√¢n t√≠ch c√∫ ph√°p XML
    tree = ET.fromstring(xml_content)
    root = ET.fromstring(xml_content)
    find_wordart_texts(root)
    # Iterate through each child of the root element
    
    
    # T√¨m t·∫•t c·∫£ c√°c section (w:sectPr)
    # L∆∞u √Ω: C√°c section th∆∞·ªùng xu·∫•t hi·ªán ·ªü cu·ªëi document ho·∫∑c gi·ªØa c√°c paragraph
    # for sect in tree.iter("{http://schemas.openxmlformats.org/wordprocessingml/2006/main}sectPr"):
    #     cols = sect.find("w:cols", namespaces)
    #     if cols is not None:
    #         num_cols = cols.attrib.get("{http://schemas.openxmlformats.org/wordprocessingml/2006/main}num")
    #         if num_cols == "2":
    #             print("Section 2 col.")
    #         else:
    #             print(f"Section 1 col: {num_cols}")
    #     else:
    #         print("Section not col.")
    for para in tree.findall(".//w:p", namespaces):
    # L·∫•y ph·∫ßn t·ª≠ pPr (paragraph properties)
        pPr = para.find("w:pPr", namespaces)
        if pPr is not None:
            drop_cap = pPr.find("w:dropCap", namespaces)
            if drop_cap is not None:
                print("Drop cap found in paragraph! Type:")
                drop_val = drop_cap.attrib.get(f"{{{namespaces['w']}}}val", "none")
                if drop_val in ("drop", "margin"):
                    print("Drop cap found in paragraph! Type:", drop_val)

# S·ª≠ d·ª•ng h√†m tr√™n v·ªõi file Word c·ªßa b·∫°n
docx_path = r"D:\doAnTuChamDiemFileWordExcel\dropcap.docx"
# sections = extract_sections(docx_path)

# In n·ªôi dung t·ª´ng section ra
# for idx, section in enumerate(sections, start=1):
#     print(f"--- Section {idx} ---")
#     for para in section:
#         print(para)
#     print("\n")
check_two_columns(docx_path)